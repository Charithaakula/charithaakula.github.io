<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sai Charitha Akula</title>
  <!-- This file is meant to drop into the jonbarron.github.io template repo. It relies on the existing stylesheet.css -->
  <link rel="stylesheet" href="stylesheet.css" />
  <style>
    /* minimal fixes if stylesheet.css differs */
    body { max-width: 900px; margin: 40px auto; padding: 0 16px; }
    .top { display: grid; grid-template-columns: 1fr 160px; gap: 24px; align-items: center; }
    img.headshot { width: 160px; height: 160px; object-fit: cover; border-radius: 8px; }
    h1 { margin: 0 0 4px 0; }
    .muted { color: #666; }
    .section { margin: 28px 0; }
    .list li { margin: 6px 0; }
    .right { text-align: right; }
    .pill { display:inline-block; padding:2px 8px; border:1px solid #ddd; border-radius:999px; font-size: 12px; margin-left:6px; }
  </style>
</head>
<body>
  <div class="top">
    <div>
      <h1>Sai Charitha Akula</h1>
      <p>
        Research Scientist — Multimodal Understanding & Generative AI<br/>
        <a href="mailto:sca321@nyu.edu">sca321@nyu.edu</a> · <a href="https://github.com/Charithaakula">GitHub</a> ·
        <a href="https://www.linkedin.com/in/sai-charitha-akula-32574887/">LinkedIn</a>
      </p>
      <p class="muted">
        I build data-centric and evaluation-centric pipelines for multimodal LLMs: captioners for billion-scale image/video pretraining, rubric-based auto-evaluation, and retrieval/agent systems that measurably improve model quality and product reliability.
      </p>
    </div>
    <div class="right">
      <!-- Replace images/headshot.jpg with your own file placed in the repo's images/ folder -->
      <img class="headshot" src="images/headshot.jpg" alt="Sai Charitha Akula" />
    </div>
  </div>

  <div class="section">
    <h2>News</h2>
    <ul class="list">
      <li>Dec 2024 — <em>Cambrian‑1</em> accepted to NeurIPS (Oral).</li>
      <li>Mar 2025 — Joined Meta Superintelligence Labs as Research Scientist (Multimedia Understanding).</li>
    </ul>
  </div>

  <div class="section">
    <h2>Publications</h2>
    <ul class="list">
      <li>
        <strong>Cambrian‑1: A fully open, vision‑centric exploration of multimodal LLMs</strong>.
        <span class="muted">NeurIPS (Oral), 2024.</span>
        <br/>
        Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, <u>Sai Charitha Akula</u>, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, Saining Xie.
        [<a href="https://arxiv.org/abs/2406.16860">arXiv</a>]
      </li>
      <li>
        <strong>A Few Adversarial Tokens Can Break Vision Transformers</strong>.
        <span class="muted">CVPR Workshop on Adversarial ML in CV, 2023.</span>
        <br/>
        <u>Sai Charitha Akula*</u>, Ameya Joshi*, Gauri Jagatap, Chinmay Hegde. [<a href="https://robustart.github.io/long_paper/31.pdf">pdf</a>]
      </li>
    </ul>
  </div>

  <div class="section">
    <h2>Experience</h2>
    <ul class="list">
      <li>
        <strong>Meta Superintelligence Labs</strong>, Research Scientist — Multimedia Understanding <span class="muted">(Mar 2025–Present)</span>
        <ul class="list">
          <li>Improved multimodal model quality via advanced post‑training (<span class="pill">SFT</span> <span class="pill">RL</span>) and reward modeling, coupled with dataset/benchmark curation and robust auto/human eval pipelines.</li>
          <li>Led <em>Understanding for Generation</em>: trained lightweight expert captioners via distillation to annotate billions of images/videos; emphasized attributes, spatial relations, style, camera, lighting, and motion with timestamps; delivered strong human‑eval gains.</li>
          <li>Launched rubric‑based auto‑evaluation framework adopted for Llama pretraining; increased benchmark difficulty and discriminative power vs. prior internal baselines.</li>
          <li>Piloted tool‑calling for MLLMs with reasoning‑trace data; defined Tool‑Need Precision/Recall and improved landmark recognition accuracy.</li>
        </ul>
      </li>
      <li>
        <strong>Millennium</strong>, AI Research Engineer <span class="muted">(Jun 2024–Mar 2025)</span>
        <ul class="list">
          <li>Shipped scalable enterprise AI systems (production RAG, 3rd‑party API integrations, custom LLM workflows); adopted by &gt;50% of the firm.</li>
          <li>Improved retrieval & generation through query understanding/routing, reranking, and graph‑based knowledge; +6–46% context precision and +13–52% context recall on internal evals.</li>
        </ul>
      </li>
      <li>
        <strong>CILVR Lab, NYU</strong>, Deep Learning Researcher (advised by Saining Xie & Mengye Ren) <span class="muted">(Apr 2023–Jun 2024)</span>
        <ul class="list">
          <li>Co‑developed data engine for <em>Cambrian‑1</em>; generated knowledge‑based visual instruction data using targeted retrieval and LLM‑assisted parsing.</li>
          <li>Built CV‑Bench to evaluate 2D spatial relations, counting, and 3D depth in a VQA format.</li>
          <li>Explored diffusion‑model efficiency (masking/contrastive learning) and joint generative‑discriminative pretraining.</li>
        </ul>
      </li>
      <li>
        <strong>Goldman Sachs — Interest Rates Desk</strong>, VP/Associate/Analyst, Quant Research & Engineering <span class="muted">(Sep 2017–Sep 2023)</span>
        <ul class="list">
          <li>Led 4‑person team in the firm‑wide LIBOR→RFR transition; delivered pricing/risk model updates and data pipelines across desks.</li>
          <li>Cut pricing time 10× by optimizing input‑curve construction using Bayesian ML and convex optimization (Innovation of the Year, 2nd runner‑up).</li>
        </ul>
      </li>
    </ul>
  </div>

  <div class="section">
    <h2>Selected Projects</h2>
    <ul class="list">
      <li><strong>Market Flux Finder</strong> — financial browser with RAG + LLMs for query‑specific extraction/summarization; CloudSQL, GKE, GPU nodes, gRPC, Prometheus. [<a href="https://github.com/ChanukyaVardhan/cloud-and-ml-RAG">code</a>]</li>
      <li><strong>Efficient Group Preference Alignment</strong> — two‑stage aligners/adapters for group‑specific tuning (RLHF/DPO/PPO). [paper]</li>
      <li><strong>Perceiver‑Resampler Cross‑Attention Captioning</strong> — CLIP+GPT‑2 bridged with Perceiver‑IO and gated x‑attn; 31.21 BLEU‑4 on COCO‑val. [<a href="https://github.com/shan18/Perceiver-Resampler-XAttn-Captioning">code</a>]</li>
      <li><strong>Video Moment Localization</strong> — iterative attention and multi‑level cross‑modal interactions for temporal grounding. [<a href="https://github.com/ChanukyaVardhan/Video-Moment-Localization">code</a>]</li>
    </ul>
  </div>

  <div class="section">
    <h2>Education</h2>
    <ul class="list">
      <li><strong>New York University (Courant)</strong> — M.S. Computer Science, 4.0/4.0 <span class="muted">(Jan 2022–May 2024)</span></li>
      <li><strong>IIT Bombay</strong> — B.Tech. Computer Science (Honours); Minor in Electrical Engineering <span class="muted">(2013–2017)</span></li>
    </ul>
  </div>

  <div class="section">
    <h2>Service & Misc</h2>
    <ul class="list">
      <li>Founder, NGO: built WhatsApp triage chatbot mitigating COVID bed surge; scaled to 100k+ concurrent requests.</li>
      <li>Skills: Python, C/C++, SQL; PyTorch/Lightning, JAX, TensorFlow; Docker, Kubernetes, GCP; Ray; gRPC; microservices.</li>
    </ul>
  </div>

  <div class="section muted">
    <p>Last updated: Nov 2025 · Template adapted from <a href="https://github.com/jonbarron/jonbarron.github.io">jonbarron.github.io</a>.</p>
  </div>
</body>
</html>
