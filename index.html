<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sai Charitha Akula</title>
  <!-- This file is meant to drop into the jonbarron.github.io template repo. It relies on the existing stylesheet.css -->
  <link rel="stylesheet" href="stylesheet.css" />
  <style>
    /* minimal fixes if stylesheet.css differs */
    body { max-width: 900px; margin: 40px auto; padding: 0 16px; }
    .top { display: grid; grid-template-columns: 1fr 160px; gap: 24px; align-items: center; }
    img.headshot { width: 160px; height: 160px; object-fit: cover; border-radius: 8px; }
    h1 { margin: 0 0 4px 0; }
    .muted { color: #666; }
    .section { margin: 28px 0; }
    .list { padding-left: 20px; }
    .list li { margin: 6px 0; }
    .right { text-align: right; }
    .linkbar a { margin-right: 8px; text-decoration: none; }

    /* project + publication layout */
    .projects, .pubs { list-style: none; padding-left: 0; }
    .project, .pub {
      display: grid;
      grid-template-columns: 130px 1fr;
      gap: 16px;
      margin-bottom: 18px;
      align-items: start;
    }
    .project img, .pub img {
      width: 130px;
      height: 90px;
      object-fit: cover;
      border-radius: 4px;
      background: #eee; /* placeholder background */
    }
    .project-title, .pub-title { font-weight: bold; }
    .project-meta, .pub-meta { font-size: 14px; color: #666; }
  </style>
</head>
<body>
  <div class="top">
    <div>
      <h1>Sai Charitha Akula</h1>
      <!-- <p>
        Research Scientist, Meta AI
      </p> -->
      <p class="linkbar">
        <a href="mailto:sca321@nyu.edu">Email</a> /
        <a href="saicharitha_akula_resume.pdf">CV</a> /
        <a href="https://github.com/Charithaakula">GitHub</a> /
        <a href="https://www.linkedin.com/in/sai-charitha-akula-32574887/">LinkedIn</a>
      </p>
      <p class="muted">
        <!-- I am a Research Scientist at Meta Superintelligence Labs, where I drive improvements to multi modal large language models via post training, focusing on improving reasoning and understanding across text, image, and video.
        Before Meta, I built enterprise-grade RAG systems and agentic workflows at Millennium and conducted research on retrieval relevance. Prior to that, I did research on
        vision-centric multimodal models and diffusion models at NYU's CILVR Lab with Prof. Saining Xie.
        I previously spent six years as a Quantitative Researcher at Goldman Sachs' Interest Rates desk,
        building ML models for automated trading, pricing, and risk management. -->

        I am a Research Scientist at Meta Superintelligence Labs, where I advance multimodal large language models through post-training techniques, with a focus on enhancing reasoning and understanding across text, image, and video modalities.

Prior to Meta, I developed enterprise-scale RAG systems and agentic workflows at Millennium while conducting research on retrieval relevance. Before that, I researched vision-centric multimodal models and diffusion models at NYU's CILVR Lab under Prof. Saining Xie.

I previously spent six years as a Quantitative Researcher at Goldman Sachs' Interest Rates desk, where I built models for automated trading, pricing, and risk management.

        <!-- Before Meta, I developed enterprise-grade retrieval and agent systems at Millennium and worked on vision-centric
        multimodal models at NYU’s CILVR Lab with Prof. Saining Xie. I previously spent six years as a Quantitative
        Researcher and Developer on Goldman Sachs’ Interest Rates desk, building machine learning models for automated trading and pricing & risk modeling. -->
      </p>
      <p class="muted">
        I completed my master's in Computer Science from NYU (Courant, https://cims.nyu.edu/dynamic/) and bachelor's in Computer Science (Honours) from Indian Institute of Technology, Bombay (https://www.iitb.ac.in/).
      </p>
    </div>
    <div class="right">
      <!-- Replace images/headshot.jpg with your own file placed in the repo's images/ folder -->
      <img class="headshot" src="images/headshot.jpg" alt="Sai Charitha Akula" />
    </div>
  </div>

  <div class="section">
    <!-- <h2>Research</h2>

    <p>

      xnMy research focuses on building reliable and capable multimodal AI systems. I work on end-to-end development
      of large-scale foundation models, with emphasis on post-training techniques including supervised fine-tuning
      and reinforcement learning, data-centric learning through synthetic data generation and curation, and robust
      evaluation methodologies. I'm particularly interested in improving model faithfulness, reasoning capabilities, and tool-use integration
      to enable AI systems that are both powerful and trustworthy in real-world applications.

      My research focuses on building reliable and adaptive AI systems, spanning multimodal foundation models,
      with an emphasis on robust and efficient post-training (across text, image, video modalities), data-centric learning, interpretable and faithful evaluation.
       tool-use, that help models reason, interpret, and improve safely.

      My work spans both research and deployment, from improving model reasoning and perception
      at Meta, to retrieval-augmented workflows and agentic systems used in high-stakes financial environments.
    </p> -->
    <!-- <p>

      At Meta, Post-training stack (SFT, RLHF/PPO, DPO/GRPO) for Llama’s multimodal understanding,
      coupled with lightweight expert captioners that annotate billions of images/videos and
      rubric-based auto-evaluation that improves fine-grained perception and reasoning.

      My research focuses on building reliable, adaptive AI systems — spanning multimodal foundation models,
      post-training and alignment, data-centric learning, and evaluation. I work across text, image, and video
      modalities, with an emphasis on robust post-training (SFT, RLHF, DPO/GRPO), tool-use, and rubric-based
      auto-evaluation frameworks that help models reason, interpret, and improve safely.
    </p>
    <p>
      More broadly, I’m interested in how AI systems can generalize across domains (e.g., vision, language,
      finance, and healthcare), adapt to real-world constraints, and solve complex reasoning tasks with fidelity
      and traceability. My work bridges research and deployment — from improving model reasoning and perception
      at Meta, to retrieval-augmented workflows and agentic systems used in high-stakes financial environments.
    </p> -->
  </div>

  <!-- <div class="section">
    <h2>News</h2>
    <ul class="list">
      <li>Mar 2025 — Joined Meta Superintelligence Labs as Research Scientist (Llama Multimodal Reasoning &amp; Post-Training).</li>
      <li>Dec 2024 — <em>Cambrian-1</em> accepted as an Oral at NeurIPS 2024.</li>
      <li>Jun 2024 — Joined Millennium as AI Research Engineer (AI infra &amp; RAG for finance).</li>
    </ul>
  </div> -->

  <div class="section">
    <h2>Publications</h2>
    <ul class="pubs">
      <li class="pub">
        <!-- replace with an actual image, e.g., images/cambrian1_pub.jpg -->
        <img src="images/cambrian1_pub.jpg" alt="Cambrian-1" />
        <div>
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2406.16860">
              Cambrian-1: A fully open, vision-centric exploration of multimodal LLMs
            </a>
          </div>
          <div class="pub-meta">NeurIPS (Oral), 2024</div>
          <!-- <p>
            Vision-centric multimodal LLM with a four-encoder vision stack (CLIP, SigLIP, OpenCLIP, DINOv2) fused via
            a Spatial Vision Aggregator, achieving strong performance on OCR, chart, and fine-grained visual reasoning
            with 5× fewer training tokens than comparable baselines. Includes CV-Bench, a benchmark for 2D spatial
            relations, counting, and 3D depth in VQA format.
          </p> -->
          <p class="muted">
            Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu,
            <u>Sai Charitha Akula</u>, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan,
            Austin Wang, Rob Fergus, Yann LeCun, Saining Xie.
          </p>
        </div>
      </li>

      <li class="pub">
        <!-- replace with an actual image, e.g., images/adv_tokens_vit.jpg -->
        <img src="images/adv_tokens_vit.jpg" alt="Adversarial Tokens on ViTs" />
        <div>
          <div class="pub-title">
            <a href="https://robustart.github.io/long_paper/31.pdf">
              A Few Adversarial Tokens Can Break Vision Transformers
            </a>
          </div>
          <div class="pub-meta">CVPR Workshop on Adversarial ML in CV, 2023</div>
          <!-- <p>
            Shows that a small number of adversarial tokens can severely degrade the performance of
            Vision Transformers, revealing vulnerabilities in token-based architectures and highlighting the need for
            more robust training and evaluation for vision models.
            Also, proposes a architectural operation to improve robustnesss.
          </p> -->
          <p class="muted">
            <u>Sai Charitha Akula*</u>, Ameya Joshi*, Gauri Jagatap, Chinmay Hegde.
          </p>
        </div>
      </li>
    </ul>
  </div>

  <div class="section">
    <h2>Selected Projects</h2>

      <li class="project">
        <!-- replace with an actual image file, e.g., images/market_flux_finder.jpg -->
        <img src="images/market_flux_finder.jpg" alt="Market Flux Finder" />
        <div>
          <div class="project-title">
            <a href="https://github.com/ChanukyaVardhan/cloud-and-ml-RAG">Market Flux Finder</a>
          </div>
          <div class="project-meta">RAG, agents, financial research</div>
          <p>
            A financial browser that uses retrieval-augmented generation and LLMs for query-specific extraction and summarization
            over large-scale firm research, built on GCP (Cloud SQL with pgvector, GKE with GPU nodes, microservices with gRPC).
          </p>
        </div>
      </li>

      <li class="project">
        <!-- replace with an actual image file, e.g., images/perceiver_captioning.jpg -->
        <img src="images/perceiver_captioning.jpg" alt="Perceiver-Resampler Captioning" />
        <div>
          <div class="project-title">
            <a href="https://github.com/shan18/Perceiver-Resampler-XAttn-Captioning">
              Perceiver-Resampler Cross-Attention Captioning
            </a>
          </div>
          <div class="project-meta">Image captioning, CLIP + GPT-2, Perceiver-IO</div>
          <p>
            Bridges CLIP and GPT-2 via Perceiver-IO and gated cross-attention, achieving strong captioning performance on COCO-val
            with a modular frozen-backbone design and a trainable Perceiver bridge and mapping network.
          </p>
        </div>
      </li>

      <li class="project">
        <!-- replace with an actual image file, e.g., images/video_moment_localization.jpg -->
        <img src="images/video_moment_localization.jpg" alt="Video Moment Localization" />
        <div>
          <div class="project-title">
            <a href="https://github.com/ChanukyaVardhan/Video-Moment-Localization">Video Moment Localization</a>
          </div>
          <div class="project-meta">Temporal grounding, cross-modal attention</div>
          <p>
            Temporal grounding system with iterative attention and multi-level cross-modal interactions, improving R@1 IoU=0.5
            on Charades-STA over strong baselines and providing interpretable grounding of language in video.
          </p>
        </div>
      </li>
    </ul>
  </div>

  <div class="section">
    <h2>Experience</h2>
    <ul class="list">
      <li><strong>Meta AI (Superintelligence Labs)</strong> — Senior Research Scientist, Llama Multimodal Reasoning &amp; Post-Training <span class="muted">(2025–Present)</span></li>
      <li><strong>Millennium</strong> — AI Research Engineer <span class="muted">(2024–2025)</span></li>
      <li><strong>CILVR Lab, NYU</strong> — Deep Learning Researcher <span class="muted">(2023–2024)</span></li>
      <li><strong>Goldman Sachs — Interest Rates Desk</strong> — Quantitative Research &amp; Engineering <span class="muted">(2017–2023)</span></li>
      <!-- <li><strong>NYU (Courant)</strong> — M.S. Computer Science <span class="muted">(2022–2024)</span></li>
      <li><strong>IIT Bombay</strong> — B.Tech. Computer Science (Honours); Minor in Electrical Eng. <span class="muted">(2013–2017)</span></li> -->
    </ul>
  </div>

  <div class="section muted">
    <p>Last updated: Nov 2025 · Template adapted from <a href="https://github.com/jonbarron/jonbarron.github.io">jonbarron.github.io</a>.</p>
  </div>
</body>
</html>
